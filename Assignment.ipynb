{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7eiDWcM_MC3H"
   },
   "source": [
    "# <font color='red'>Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfe2NTQtLq11"
   },
   "source": [
    "**There will be some functions that start with the word \"grader\" ex: grader_weights(), grader_sigmoid(), grader_logloss() etc, you should not change those function definition.<br><br>Every Grader function has to return True.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fk5DSPCLxqT-"
   },
   "source": [
    "<font color='red'> Importing packages</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42Et8BKIxnsp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpSk3WQBx7TQ"
   },
   "source": [
    "<font color='red'>Creating custom dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BsMp0oWzx6dv"
   },
   "outputs": [],
   "source": [
    "# please don't change random_state\n",
    "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
    "# make_classification is used to create custom dataset \n",
    "# Please check this link\n",
    "#(https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L8W2fg1cyGdX",
    "outputId": "029d4c84-03b2-4143-a04c-34ff49c88890"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 15), (50000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x99RWCgpqNHw"
   },
   "source": [
    "<font color='red'>Splitting data into train and test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Kh4dBfVyJMP"
   },
   "outputs": [],
   "source": [
    "#please don't change random state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gONY1YiDq7jD"
   },
   "outputs": [],
   "source": [
    "# Standardizing the data.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0DR_YMBsyOci",
    "outputId": "732014d9-1731-4d3f-918f-a9f5255ee149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 15), (37500,), (12500, 15), (12500,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BW4OHswfqjHR"
   },
   "source": [
    "# <font color='red' size=5>SGD classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "3HpvTwDHyQQy",
    "outputId": "5729f08c-079a-4b17-bf51-f9aeb5abb13b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha : float\n",
    "# Constant that multiplies the regularization term. \n",
    "\n",
    "# eta0 : double\n",
    "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
    "\n",
    "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf\n",
    "# Please check this documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "YYaVyQ2lyXcr",
    "outputId": "dc0bf840-b37e-4552-e513-84b64f6c64c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.70, NNZs: 15, Bias: -0.501317, T: 37500, Avg. loss: 0.552526\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.04, NNZs: 15, Bias: -0.752393, T: 75000, Avg. loss: 0.448021\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.26, NNZs: 15, Bias: -0.902742, T: 112500, Avg. loss: 0.415724\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.43, NNZs: 15, Bias: -1.003816, T: 150000, Avg. loss: 0.400895\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.55, NNZs: 15, Bias: -1.076296, T: 187500, Avg. loss: 0.392879\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.65, NNZs: 15, Bias: -1.131077, T: 225000, Avg. loss: 0.388094\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.73, NNZs: 15, Bias: -1.171791, T: 262500, Avg. loss: 0.385077\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.80, NNZs: 15, Bias: -1.203840, T: 300000, Avg. loss: 0.383074\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.86, NNZs: 15, Bias: -1.229563, T: 337500, Avg. loss: 0.381703\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.90, NNZs: 15, Bias: -1.251245, T: 375000, Avg. loss: 0.380763\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.94, NNZs: 15, Bias: -1.269044, T: 412500, Avg. loss: 0.380084\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.98, NNZs: 15, Bias: -1.282485, T: 450000, Avg. loss: 0.379607\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.01, NNZs: 15, Bias: -1.294386, T: 487500, Avg. loss: 0.379251\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.03, NNZs: 15, Bias: -1.305805, T: 525000, Avg. loss: 0.378992\n",
      "Total training time: 0.10 seconds.\n",
      "Convergence after 14 epochs took 0.10 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X_train, y=y_train) # fitting our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "EAfkVI6GyaRO",
    "outputId": "bc88f920-6531-4106-9b4c-4dabb6d72b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.89007184,  0.63162363, -0.07594145,  0.63107107, -0.38434375,\n",
       "          0.93235243, -0.89573521, -0.07340522,  0.40591417,  0.4199991 ,\n",
       "          0.24722143,  0.05046199, -0.08877987,  0.54081652,  0.06643888]]),\n",
       " (1, 15),\n",
       " array([-1.30580538]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.coef_.shape, clf.intercept_\n",
    "#clf.coef_ will return the weights\n",
    "#clf.coef_.shape will return the shape of weights\n",
    "#clf.intercept_ will return the intercept term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-CcGTKgsMrY"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "## <font color='red' size=5> Implement Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1_8bdzitDlM"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "1.  We will be giving you some functions, please write code in that functions only.\n",
    "\n",
    "2.  After every function, we will be giving you expected output, please make sure that you get that output. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zU2Y3-FQuJ3z"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "* Initialize the weight_vector and intercept term to zeros (Write your code in <font color='blue'>def initialize_weights()</font>)\n",
    "\n",
    "* Create a loss function (Write your code in <font color='blue'>def logloss()</font>) \n",
    "\n",
    " $log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
    "- for each epoch:\n",
    "\n",
    "    - for each batch of data points in train: (keep batch size=1)\n",
    "\n",
    "        - calculate the gradient of loss function w.r.t each weight in weight vector (write your code in <font color='blue'>def gradient_dw()</font>)\n",
    "\n",
    "        $dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)})$ <br>\n",
    "\n",
    "        - Calculate the gradient of the intercept (write your code in <font color='blue'> def gradient_db()</font>) <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>\n",
    "\n",
    "           $ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t}))$\n",
    "\n",
    "        - Update weights and intercept (check the equation number 32 in the above mentioned <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>pdf</a>): <br>\n",
    "        $w^{(t+1)}← w^{(t)}+α(dw^{(t)}) $<br>\n",
    "\n",
    "        $b^{(t+1)}←b^{(t)}+α(db^{(t)}) $\n",
    "    - calculate the log loss for train and test with the updated weights (you can check the python assignment 10th question)\n",
    "    - And if you wish, you can compare the previous loss and the current loss, if it is not updating, then\n",
    "        you can stop the training\n",
    "    - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZR_HgjgS_wKu"
   },
   "source": [
    "<font color='blue'>Initialize weights </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GecwYV9fsKZ9"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(dim):\n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    column = len(dim)\n",
    "    total_element = 1*column\n",
    "    w = np.arange(total_element, dtype=float)\n",
    "    # w = w.reshape((1, column))\n",
    "    w = np.zeros_like(w)\n",
    "    \n",
    "    b = 0\n",
    "    \n",
    "    print(len(w))\n",
    "\n",
    "     \n",
    "    #initialize the weights to zeros array of (1,dim) dimensions\n",
    "    #you use zeros_like function to initialize zero, \n",
    "    #check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
    "    #initialize bias to zero\n",
    "\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7I6uWBRsKc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "b = 0\n",
      "[-0.39348337 -0.19771903 -0.15037836 -0.21528098 -1.28594363 -0.66049132\n",
      "  0.04140556 -0.22680269 -0.511055   -0.42871073  0.4210912   0.22560347\n",
      " -0.6624427  -0.68888516  0.56015427]\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "w,b = initialize_weights(dim)\n",
    "print('w =',(w))\n",
    "print('b =',str(b))\n",
    "print(dim)\n",
    "print(len(dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4MI5SAjP9ofN"
   },
   "source": [
    "<font color='cyan'>Grader function - 1 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pv1llH429wG5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "w,b = initialize_weights(dim)\n",
    "def grader_weights(w,b):\n",
    "  assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
    "  return True\n",
    "grader_weights(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QN83oMWy_5rv"
   },
   "source": [
    "<font color='blue'>Compute sigmoid </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPv4NJuxABgs"
   },
   "source": [
    "$sigmoid(z)= 1/(1+exp(-z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAfmQF47_Sd6"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    # compute sigmoid(z) and return\n",
    "    sigmoid_distance = 1 / (1 + np.exp(-1*z))\n",
    "    return sigmoid_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9YrGDwg3Ae4m"
   },
   "source": [
    "<font color='cyan'>Grader function - 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_JASp_NAfK_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_sigmoid(z):\n",
    "  val=sigmoid(z)\n",
    "  assert(val==0.8807970779778823)\n",
    "  return True\n",
    "grader_sigmoid(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gS7JXbcrBOFF"
   },
   "source": [
    "<font color='blue'> Compute loss </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lfEiS22zBVYy"
   },
   "source": [
    "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VaFDgsp3sKi6"
   },
   "outputs": [],
   "source": [
    "def logloss(y_true,y_pred):\n",
    "    '''In this function, we will compute log loss '''\n",
    "    product_of_t_and_pred = 0\n",
    "    product_of_one_minus_t_and_pred = 0\n",
    "    number_of_element = len(y_true)\n",
    "    for t, pred in zip(y_true, y_pred):\n",
    "        #print(t, pred)\n",
    "        product_of_t_and_pred += t * np.log10(pred)\n",
    "        product_of_one_minus_t_and_pred += (1 - t) * np.log10(1 - pred)\n",
    "        \n",
    "    loss = -1 * (product_of_t_and_pred + product_of_one_minus_t_and_pred) / number_of_element\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zs1BTXVSClBt"
   },
   "source": [
    "<font color='cyan'>Grader function - 3 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzttjvBFCuQ5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_logloss(true,pred):\n",
    "  loss=logloss(true,pred)\n",
    "  assert(loss==0.07644900402910389)\n",
    "  return True\n",
    "true=[1,1,0,1,0]\n",
    "pred=[0.9,0.8,0.1,0.8,0.2]\n",
    "grader_logloss(true,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQabIadLCBAB"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to  'w' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTMxiYKaCQgd"
   },
   "source": [
    "$dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMVikyuFsKo5"
   },
   "outputs": [],
   "source": [
    "def sigmoid_distance(w, x, b):\n",
    "    sigmoid_distance = 1 / (1 + np.exp(-1*np.dot(x, w)+b))\n",
    "    return sigmoid_distance\n",
    "\n",
    "#xn(yn − σ((w(t))T xn)) − λ/Nw(t)\n",
    "#m=(1-((eta0*alpha)/N))*wt + eta0*X_train[i] * (y_train[i]-sigmoid(wt,X_train[i],bt))\n",
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    eta0 = alpha\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "    for xi in x:\n",
    "#         dw = (1-((eta0*alpha)/N))*w + eta0*x * (y-sigmoid_distance(w,x,b))\n",
    "        dw = (xi * ( y - sigmoid_distance(w, x, b)) - ((alpha)/N)*w)\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUFLNqL_GER9"
   },
   "source": [
    "<font color='cyan'>Grader function - 4 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WI3xD8ctGEnJ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "37500 15\n",
      "[2.77293046 2.77293046 2.77293046 2.77293046 2.77293046 2.77293046\n",
      " 2.77293046 2.77293046 2.77293046 2.77293046 2.77293046 2.77293046\n",
      " 2.77293046 2.77293046 2.77293046]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-7a1fb0bbe96f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mgrader_dw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-160-7a1fb0bbe96f>\u001b[0m in \u001b[0;36mgrader_dw\u001b[0;34m(x, y, w, b, alpha, N)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mgrad_dw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_dw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_dw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_dw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2.613689585\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def grader_dw(x,y,w,b,alpha,N):\n",
    "    grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
    "    print(grad_dw)\n",
    "    assert(np.sum(grad_dw)==2.613689585)\n",
    "    return True\n",
    "\n",
    "\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w,grad_b=initialize_weights(grad_x)\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "print(N, len(grad_x))\n",
    "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nUf2ft4EZp8"
   },
   "outputs": [],
   "source": [
    "def sigmoid_distance(w, x, b):\n",
    "    sigmoid_distance = 1 / (1 + np.exp(-1*np.dot(x, w)+b))\n",
    "    return sigmoid_distance\n",
    "\n",
    "# In this function, we will compute gradient w.r.to b \n",
    "def gradient_db(x,y,w,b):\n",
    "    db = (y - sigmoid_distance(w, x, b))\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbcBzufVG6qk"
   },
   "source": [
    "<font color='cyan'>Grader function - 5 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TfFDKmscG5qZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_db(x,y,w,b):\n",
    "  grad_db=gradient_db(x,y,w,b)\n",
    "  assert(grad_db==-0.5)\n",
    "  return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w,grad_b=initialize_weights(grad_x)\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_db(grad_x,grad_y,grad_w,grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCK0jY_EOvyU"
   },
   "source": [
    "<font color='blue'> Implementing logistic regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here eta0 is learning rate\n",
    "#implement the code as follows\n",
    "# initalize the weights (call the initialize_weights(X_train[0]) function)\n",
    "# for every epoch\n",
    "# for every data point(X_train,y_train)\n",
    "#compute gradient w.r.to w (call the gradient_dw() function)\n",
    "#compute gradient w.r.to b (call the gradient_db() function)\n",
    "#update w, b\n",
    "# predict the output of x_train[for all data points in X_train] using w,b\n",
    "#compute the loss between predicted and actual values (call the loss function)\n",
    "# store all the train loss values in a list\n",
    "# predict the output of x_test[for all data points in X_test] using w,b\n",
    "#compute the loss between predicted and actual values (call the loss function)\n",
    "# store all the test loss values in a list\n",
    "# you can also compare previous loss and current loss, \n",
    "#if loss is not updating then stop the process and return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        predict.append(sigmoid(z)) \n",
    "    return np.array(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_values = []\n",
    "test_loss_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmAdc5ejEZ25"
   },
   "outputs": [],
   "source": [
    "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "    w, b = initialize_weights(X_train[0])\n",
    "    for i in range(epochs):\n",
    "        print(w, b)\n",
    "        print(10*\"--\")\n",
    "        for x, y in zip(X_train,y_train):  \n",
    "            wi = gradient_dw(x,y,w,b,alpha,N)\n",
    "            bi = gradient_db(x,y,w,b)\n",
    "            print(wi, bi)\n",
    "#             print(wi-w)\n",
    "#             print(bi-b)\n",
    "            w = wi\n",
    "            b = bi\n",
    "            if (bi-b) == 0:\n",
    "                break\n",
    "            # print(w,b)\n",
    "            \n",
    "#         print(\"Weight and intercept\")\n",
    "#         print(w, b)\n",
    "#         y_pred = pred(w,b,X_train)\n",
    "#         print(\"Train y true\")\n",
    "#         print(y_train)\n",
    "#         print(\"Train y prediction\")\n",
    "#         print(y_pred)\n",
    "#         loss = logloss(y_train,y_pred)\n",
    "#         print(\"train loss\")\n",
    "#         print(loss)\n",
    "#         train_loss_values.append(loss)\n",
    "#         y_pred = pred(w,b,X_test)\n",
    "#         print(\"Test y prediction\")\n",
    "#         print(y_pred)\n",
    "#         loss = logloss(y_train,y_pred)\n",
    "#         print(\"test loss\")\n",
    "#         print(loss)\n",
    "#         test_loss_values.append(loss)\n",
    "#     print(\"List of train and test loss\")\n",
    "#     print(train_loss_values, test_loss_values)\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUquz7LFEZ6E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] 0\n",
      "--------------------\n",
      "[-0.28007713 -0.28007713 -0.28007713 -0.28007713 -0.28007713 -0.28007713\n",
      " -0.28007713 -0.28007713 -0.28007713 -0.28007713 -0.28007713 -0.28007713\n",
      " -0.28007713 -0.28007713 -0.28007713] -0.5\n",
      "[-0.28007713 -0.28007713 -0.28007713 -0.28007713 -0.28007713 -0.28007713\n",
      " -0.28007713 -0.28007713 -0.28007713 -0.28007713 -0.28007713 -0.28007713\n",
      " -0.28007713 -0.28007713 -0.28007713] -0.5\n",
      "--------------------\n",
      "[-0.47131849 -0.47131849 -0.47131849 -0.47131849 -0.47131849 -0.47131849\n",
      " -0.47131849 -0.47131849 -0.47131849 -0.47131849 -0.47131849 -0.47131849\n",
      " -0.47131849 -0.47131849 -0.47131849] -0.841408377614893\n",
      "[-0.47131849 -0.47131849 -0.47131849 -0.47131849 -0.47131849 -0.47131849\n",
      " -0.47131849 -0.47131849 -0.47131849 -0.47131849 -0.47131849 -0.47131849\n",
      " -0.47131849 -0.47131849 -0.47131849] -0.841408377614893\n",
      "--------------------\n",
      "[-0.52829094 -0.52829094 -0.52829094 -0.52829094 -0.52829094 -0.52829094\n",
      " -0.52829094 -0.52829094 -0.52829094 -0.52829094 -0.52829094 -0.52829094\n",
      " -0.52829094 -0.52829094 -0.52829094] -0.9431168723672787\n",
      "[-0.52829094 -0.52829094 -0.52829094 -0.52829094 -0.52829094 -0.52829094\n",
      " -0.52829094 -0.52829094 -0.52829094 -0.52829094 -0.52829094 -0.52829094\n",
      " -0.52829094 -0.52829094 -0.52829094] -0.9431168723672787\n",
      "--------------------\n",
      "[-0.53708472 -0.53708472 -0.53708472 -0.53708472 -0.53708472 -0.53708472\n",
      " -0.53708472 -0.53708472 -0.53708472 -0.53708472 -0.53708472 -0.53708472\n",
      " -0.53708472 -0.53708472 -0.53708472] -0.9588157272152771\n",
      "[-0.53708472 -0.53708472 -0.53708472 -0.53708472 -0.53708472 -0.53708472\n",
      " -0.53708472 -0.53708472 -0.53708472 -0.53708472 -0.53708472 -0.53708472\n",
      " -0.53708472 -0.53708472 -0.53708472] -0.9588157272152771\n",
      "--------------------\n",
      "[-0.5382162 -0.5382162 -0.5382162 -0.5382162 -0.5382162 -0.5382162\n",
      " -0.5382162 -0.5382162 -0.5382162 -0.5382162 -0.5382162 -0.5382162\n",
      " -0.5382162 -0.5382162 -0.5382162] -0.9608356774459106\n",
      "[-0.5382162 -0.5382162 -0.5382162 -0.5382162 -0.5382162 -0.5382162\n",
      " -0.5382162 -0.5382162 -0.5382162 -0.5382162 -0.5382162 -0.5382162\n",
      " -0.5382162 -0.5382162 -0.5382162] -0.9608356774459106\n",
      "--------------------\n",
      "[-0.53835787 -0.53835787 -0.53835787 -0.53835787 -0.53835787 -0.53835787\n",
      " -0.53835787 -0.53835787 -0.53835787 -0.53835787 -0.53835787 -0.53835787\n",
      " -0.53835787 -0.53835787 -0.53835787] -0.9610885789945057\n",
      "[-0.53835787 -0.53835787 -0.53835787 -0.53835787 -0.53835787 -0.53835787\n",
      " -0.53835787 -0.53835787 -0.53835787 -0.53835787 -0.53835787 -0.53835787\n",
      " -0.53835787 -0.53835787 -0.53835787] -0.9610885789945057\n",
      "--------------------\n",
      "[-0.53837554 -0.53837554 -0.53837554 -0.53837554 -0.53837554 -0.53837554\n",
      " -0.53837554 -0.53837554 -0.53837554 -0.53837554 -0.53837554 -0.53837554\n",
      " -0.53837554 -0.53837554 -0.53837554] -0.9611201321542424\n",
      "[-0.53837554 -0.53837554 -0.53837554 -0.53837554 -0.53837554 -0.53837554\n",
      " -0.53837554 -0.53837554 -0.53837554 -0.53837554 -0.53837554 -0.53837554\n",
      " -0.53837554 -0.53837554 -0.53837554] -0.9611201321542424\n",
      "--------------------\n",
      "[-0.53837775 -0.53837775 -0.53837775 -0.53837775 -0.53837775 -0.53837775\n",
      " -0.53837775 -0.53837775 -0.53837775 -0.53837775 -0.53837775 -0.53837775\n",
      " -0.53837775 -0.53837775 -0.53837775] -0.9611240671484268\n",
      "[-0.53837775 -0.53837775 -0.53837775 -0.53837775 -0.53837775 -0.53837775\n",
      " -0.53837775 -0.53837775 -0.53837775 -0.53837775 -0.53837775 -0.53837775\n",
      " -0.53837775 -0.53837775 -0.53837775] -0.9611240671484268\n",
      "--------------------\n",
      "[-0.53837802 -0.53837802 -0.53837802 -0.53837802 -0.53837802 -0.53837802\n",
      " -0.53837802 -0.53837802 -0.53837802 -0.53837802 -0.53837802 -0.53837802\n",
      " -0.53837802 -0.53837802 -0.53837802] -0.9611245578547045\n",
      "[-0.53837802 -0.53837802 -0.53837802 -0.53837802 -0.53837802 -0.53837802\n",
      " -0.53837802 -0.53837802 -0.53837802 -0.53837802 -0.53837802 -0.53837802\n",
      " -0.53837802 -0.53837802 -0.53837802] -0.9611245578547045\n",
      "--------------------\n",
      "[-0.53837805 -0.53837805 -0.53837805 -0.53837805 -0.53837805 -0.53837805\n",
      " -0.53837805 -0.53837805 -0.53837805 -0.53837805 -0.53837805 -0.53837805\n",
      " -0.53837805 -0.53837805 -0.53837805] -0.9611246190469195\n",
      "[-0.53837805 -0.53837805 -0.53837805 -0.53837805 -0.53837805 -0.53837805\n",
      " -0.53837805 -0.53837805 -0.53837805 -0.53837805 -0.53837805 -0.53837805\n",
      " -0.53837805 -0.53837805 -0.53837805] -0.9611246190469195\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246266777248\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246266777248\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246276293031\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246276293031\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277479669\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277479669\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277627646\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277627646\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277646098\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277646098\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.96112462776484\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.96112462776484\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648687\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648687\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648722\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648722\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648726\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648726\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n",
      "--------------------\n",
      "[-0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806 -0.53837806\n",
      " -0.53837806 -0.53837806 -0.53837806] -0.9611246277648728\n"
     ]
    }
   ],
   "source": [
    "alpha=0.0001\n",
    "eta0=0.0001\n",
    "N=len(X_train)\n",
    "epochs=50\n",
    "w,b=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802,\n",
       "  0.2854748283156802],\n",
       " [0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285,\n",
       "  0.0963365095989285])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_values, test_loss_values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4Zf_wPARlwY"
   },
   "source": [
    "<font color='red'>Goal of assignment</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3eF_VSPSH2z"
   },
   "source": [
    "Compare your implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in terms of 10^-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nx8Rs9rfEZ1R"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.91328792, -0.60840755,  0.09915753, -0.60785499,  0.40755983,\n",
       "         -0.90913635,  0.91895129,  0.0966213 , -0.38269809, -0.39678302,\n",
       "         -0.22400535, -0.0272459 ,  0.11199595, -0.51760044, -0.0432228 ]]),\n",
       " array([1.14883265]))"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the results we got after we implemented sgd and found the optimal weights and intercept\n",
    "w-clf.coef_, b-clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "230YbSgNSUrQ"
   },
   "source": [
    "<font color='blue'>Plot epoch number vs train , test loss </font>\n",
    "\n",
    "* epoch number on X-axis\n",
    "* loss on Y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1O6GrRt7UeCJ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Taken idea from Naive Bayes Assignment\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "epochlst=np.arange(0,epochs)\n",
    "plt.plot(epochlst, train_loss_values, label='Train Epoch learning curve')\n",
    "plt.scatter(epochlst, train_loss_values)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Log Loss\")\n",
    "plt.title(\"Learning Graph\")\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochlst, test_loss_values, label='Test Epoch learning curve')\n",
    "plt.scatter(epochlst, test_loss_values)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Log Loss\")\n",
    "plt.title(\"Learning Graph\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUN8puFoEZtU"
   },
   "outputs": [],
   "source": [
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        if sigmoid(z) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
    "            predict.append(1)\n",
    "        else:\n",
    "            predict.append(0)\n",
    "    return np.array(predict)\n",
    "print(1-np.sum(y_train - pred(w,b,X_train))/len(X_train))\n",
    "print(1-np.sum(y_test  - pred(w,b,X_test))/len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-k28U1xDsLIO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMokBfs3-2PY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
